\documentclass{eplDoc}
\usepackage{placeins}


\newcommand{\docType}	{Assignment 3 : Semantic word clustering}
\newcommand{\docDate}	{03/05/2012}
\newcommand{\docAuthor}	{gr10 : Mulders Corentin, Pelsser Francois}
\newcommand{\courseCode}{LINGI2263}
\newcommand{\courseName}{Computational linguistics}
\usepackage{syntax}
\begin{document}
\maketitle
\newpage

\section{Global architecture}

Our program is composed of three parts : 
\begin{enumerate}
	\item Lemmatization and part of speech tagging (tag.py)
	\item Filtering of stop words and stop part-of-speech tags (filter.py)
	\item Clustering  %TODO py name
\end{enumerate}

The first two parts do the preprocessing on the text files. The first one takes a definitions file as input and produces a file with words in the definitions tagged with a part-of-speech tag and lemmatized. The second takes a file with tagged definitions and removes words that match either a stop word or a stop tag. \\ 

The third part is the main part, it takes a preprocessed file as input and produces an output file with the culsters of words. \\

This part is composed of two datastructures (definition and dictionary) one computation file (skmean) and the file that launch everything (cluster)\\
We wanted cluster to be or launched automatically after the preprocess or alone (to avoid preprocessing every time).  This allow to test multiple K.
First cluster read the preprocessed file, create a definition object for every definition and add it in dictionary.
Then the dictionary compute the idf vector and the definition objects compute the whole tf-idf vector for every definition.
\\
Everything is then passed to the skmean algorithm which compute the different clusters.\\
One particularity of our implementation is that the tf-idf are stored as dictionaries which allow to save memory because a definition only contains few words from the whole vocabulary.
\\
We decided to initialise the k centroid vectors with the k first definition vectors.  In this way we are sure that there is no empty clusters wich could lead to additive iteration needed.

\subsection{Lemmatization and part of speech tagging}
This part of the program uses TreeTagger to tag and lemmatize the definitions.

\subsubsection{Filtering of stop words and stop part-of-speech tags}

We filter the stop words from a common stop words list that we retrieved on the internet. \\ 
We also filter words based on some POS tags : 
\begin{itemize}
	\item CC, that are the coordinating conjunctions.
	\item DT, the determiners.
	\item IN, the prepositions or subordinating conjunctions.
	\item SENT, the end of sentences.
\end{itemize}

\section{Discussion on the results}

\subsection{Results depending on the value of K}
For very small values of K the results don't seem better at all since all the words are put in huge cluster in which they don't seem to be related at all. However when the value of K approaches the number of words the quality of the results deteriorate since we get clusters with only one or two words. \\ 

The values of K that seemed to be the best to us are values around 1000-1500. We get clusters containing around 15 words that seem to belogn together. We will chose the values of $K=1000$ and $K=1500$, the reason is that below 1000 we get too big clusters with a high rate of occurences of unrelated words being put together. And above 1500 the clusters get too small and we begin to have a lot of cluster scontaining only one word. \\ 
The results for those two values are available in the attached files. 

\subsection{How to automate the evaluation of the quality of the clustering} %and ressources needed
We could use thematic dictionaris and synonym dictionaries.  Then look how many synonys are in the same cluster and how many are in different, or how is the probability to 2 words from the same theme to be in the same cluster.


\subsection{Possible ways to enhance our results} % and reason for error sin our results
We could use more sources.  We have definitions with only one word.  This is not very representative.  We could also add the defined word to its definition.

\end{document}
