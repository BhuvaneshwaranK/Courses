\documentclass{eplDoc}
\usepackage{placeins}


\newcommand{\docType}	{Assignment 3 : Semantic word clustering}
\newcommand{\docDate}	{03/05/2012}
\newcommand{\docAuthor}	{gr10 : Mulders Corentin, Pelsser Francois}
\newcommand{\courseCode}{LINGI2263}
\newcommand{\courseName}{Computational linguistics}
\usepackage{syntax}
\begin{document}
\maketitle
\newpage

\section{Global architecture}

Our program is composed of three parts : 
\begin{enumerate}
	\item Lemmatization and part of speech tagging (tag.py)
	\item Filtering of stop words and stop part-of-speech tags (filter.py)
	\item Clustering (cluster.py) 
\end{enumerate}

Each of these parts can be executed separately to perform its task or the three can be used as a whole using semantic_words_clustering.py. \\ 

The first two parts do the preprocessing on the text files. The first one takes a definitions file as input and produces a file with words in the definitions tagged with a part-of-speech tag and lemmatized. The second takes a file with tagged definitions and removes words that match either a stop word or a stop tag. \\ 

The third part is the main part, it takes a preprocessed file as input and produces an output file with the culsters of words. \\

This part is composed of two datastructures (definition and dictionary) one computation file (skmean) and the file that launch everything (cluster)\\
We wanted cluster to be or launched automatically after the preprocess or alone (to avoid preprocessing every time).  This allow to test multiple K.
First cluster read the preprocessed file, create a definition object for every definition and add it in dictionary.
Then the dictionary compute the idf vector and the definition objects compute the whole tf-idf vector for every definition.
\\
Everything is then passed to the skmean algorithm which compute the different clusters.\\
One particularity of our implementation is that the tf-idf are stored as dictionaries which allow to save memory because a definition only contains few words from the whole vocabulary.
\\
We decided to initialise the k centroid vectors with the k first definition vectors.  In this way we are sure that there is no empty clusters wich could lead to additive iteration needed.

\subsection{Lemmatization and part of speech tagging}
This part of the program uses TreeTagger to tag and lemmatize the definitions.

\subsubsection{Filtering of stop words and stop part-of-speech tags}

We filter the stop words from a common stop words list that we retrieved on the internet. \\ 
We also filter words based on some POS tags : 
\begin{itemize}
	\item CC, that are the coordinating conjunctions.
	\item DT, the determiners.
	\item IN, the prepositions or subordinating conjunctions.
	\item SENT, the end of sentences.
\end{itemize}

\section{Discussion on the results}

\subsection{Results depending on the value of K}
For very small values of K the results don't seem better at all since all the words are put in huge cluster in which they don't seem to be related at all. However when the value of K approaches the number of words the quality of the results deteriorate since we get clusters with only one or two words. \\ 

The values of K that seemed to be the best to us are values around 1000-1500. We get clusters containing around 15 words that seem to belogn together. We will chose the values of $K=1000$ and $K=1500$, the reason is that below 1000 we get too big clusters with a high rate of occurences of unrelated words being put together. And above 1500 the clusters get too small and we begin to have a lot of cluster scontaining only one word. \\ 
The results for those two values are available in the attached files. 

\subsection{How to automate the evaluation of the quality of the clustering} %and ressources needed
We could use a them based words database and synonym dictionaries. Or use a different clustering algorithm or use the same algorithm with more extensive definitions. \\ 
Then we might use this as a reference to check whether or not a word in a cluster belongs with the other members of the cluster and count the number of misplaced words which would give us an evaluation of how bad our results are. 


\subsection{Possible ways to enhance our results} % and reason for error sin our results
We could use more sources.  We have definitions with only one word.  This is not very representative.  We could also add the defined word to its definition.

\section{How to use our program}
To use our program you need to have installed TreeTagger and made sure it works on your platform. The TreeTagger executable (the script that can be called with only an input file and an output file, not the main executable) must either have the standard name for windows or linux and be in the path or be specified in the command line to our program. \\ 
The program detects if the input file is already tagged to avoid doing the preprocessing twice by searching for the key "tagged" in the file name. \\ 
The value of K for the algorithm can be specified and is set to 1500 by default. \\
You also need python 3 to use our program. \\ 
Here is the syntax to us eour program : 
\lstset{breaklines=true}
\begin{lstlisting}
python semantic_words_clustering.py definitions_file clusters_file [k_value [treetagger_executabe]]
\end{lstlisting}



\end{document}
